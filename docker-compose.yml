version: '3'

services:

  server:
    build:
      context: server
      args:
        - MODEL_NAME=identity
    environment:
      - MODEL_NAME=identity
      - SERVING_PORT=8500
    ports:
      - "8500:8500"

  wsgi-client:
    build:
      context: client
      args:
        - MODEL_NAME=identity
    command:
      gunicorn --workers=4 --bind=0.0.0.0:8000 wsgi_client:api
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
      - CLIENT_PORT=8000
    ports:
      - "8000:8000"
    links:
      - server

  tornado-client:
    build:
      context: client
      args:
        - MODEL_NAME=identity
    command:
      python3 tornado_client.py --client_port=8000
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
      - CLIENT_PORT=8000
    ports:
      - "8000:8000"
    links:
      - server

  grpc-benchmark:
    build:
      context: client
      args:
        - MODEL_NAME=identity
    command:
      python3 grpc_client.py --num_requests=10000 --max_concurrent=10
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
    links:
      - server

  wsgi-benchmark:
    build:
      context: client
    command:
      ab -n 10000 -c 10 http://client:8000/prediction
    links:
      - server
      - wsgi-client:client

  tornado-benchmark:
    build:
      context: client
    command:
      ab -n 10000 -c 10 http://client:8000/prediction
    links:
      - server
      - tornado-client:client